{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNLJOqu9YUoRswlCu9RcWBI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ochamodev/data_science_at_work_project_final/blob/main/DS_Research_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lib imports"
      ],
      "metadata": {
        "id": "BqEPw2o4Lswp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# library imports\n",
        "\n",
        "    # utils\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "    # data wrangling and graphs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colormaps\n",
        "\n",
        "    # dataset balancing\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
        "\n",
        "    # models, metrics and feature selection from sklearn\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_squared_error, mean_absolute_error\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "    # tensorflow for DL\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "    # keras layers and models\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model, Sequential\n",
        "from keras import regularizers"
      ],
      "metadata": {
        "id": "5s1YHBJCLukF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load dataset"
      ],
      "metadata": {
        "id": "Tsjn2h-VLxwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Datasest found @\n",
        "https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
        "'''\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# copy to local dir\n",
        "destination = \"/content/dataset\"\n",
        "shutil.copytree(path, destination)\n",
        "\n",
        "# read dataset\n",
        "df = pd.read_csv(\"/content/dataset/creditcard.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRq3ObOPLxdG",
        "outputId": "6e9495e5-f3a7-4847-a125-e5f1340b0067"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mlg-ulb/creditcardfraud?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66.0M/66.0M [00:02<00:00, 30.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/mlg-ulb/creditcardfraud/versions/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# autoencoder original chatio"
      ],
      "metadata": {
        "id": "2vmtEOypL7th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(203)\n",
        "\n",
        "data = pd.read_csv(\"/content/dataset/creditcard.csv\")\n",
        "data[\"Time\"] = data[\"Time\"].apply(lambda x : x / 3600 % 24)\n",
        "\n",
        "non_fraud = data[data['Class'] == 0].sample(1000)\n",
        "fraud = data[data['Class'] == 1]\n",
        "\n",
        "df = pd.concat([non_fraud, fraud]).sample(frac=1).reset_index(drop=True)\n",
        "X = df.drop(['Class'], axis = 1).values\n",
        "Y = df[\"Class\"].values\n",
        "\n",
        "# input layer\n",
        "input_layer = Input(shape=(X.shape[1],))\n",
        "\n",
        "# encoding part\n",
        "encoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
        "encoded = Dense(50, activation='relu')(encoded)\n",
        "\n",
        "# decoding part\n",
        "decoded = Dense(50, activation='tanh')(encoded)\n",
        "decoded = Dense(100, activation='tanh')(decoded)\n",
        "\n",
        "# output layer\n",
        "output_layer = Dense(X.shape[1], activation='relu')(decoded)\n",
        "\n",
        "# model architecture\n",
        "autoencoder = Model(input_layer, output_layer)\n",
        "autoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")\n",
        "\n",
        "x = data.drop([\"Class\"], axis=1)\n",
        "y = data[\"Class\"].values\n",
        "x_scale = preprocessing.MinMaxScaler().fit_transform(x.values)\n",
        "x_norm, x_fraud = x_scale[y == 0], x_scale[y == 1]\n",
        "\n",
        "autoencoder.fit(x_norm[0:2000], x_norm[0:2000],\n",
        "                batch_size = 256, epochs = 10,\n",
        "                shuffle = True, validation_split = 0.20);\n",
        "\n",
        "# Latent Representations\n",
        "hidden_representation = Sequential()\n",
        "hidden_representation.add(autoencoder.layers[0])\n",
        "hidden_representation.add(autoencoder.layers[1])\n",
        "hidden_representation.add(autoencoder.layers[2])\n",
        "\n",
        "# Generate the hidden representations of two classes : non-fraud and fraud by predicting the raw inputs\n",
        "norm_hid_rep = hidden_representation.predict(x_norm[:3000])\n",
        "fraud_hid_rep = hidden_representation.predict(x_fraud)\n",
        "\n",
        "# create a training dataset using the latent representations\n",
        "rep_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)\n",
        "y_n = np.zeros(norm_hid_rep.shape[0])\n",
        "y_f = np.ones(fraud_hid_rep.shape[0])\n",
        "rep_y = np.append(y_n, y_f)\n",
        "\n",
        "# Linear Classifier\n",
        "train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25)\n",
        "clf = LogisticRegression(solver=\"lbfgs\").fit(train_x, train_y)\n",
        "pred_y = clf.predict(val_x)\n",
        "\n",
        "print (\"\")\n",
        "print (\"Classification Report: \")\n",
        "print (classification_report(val_y, pred_y))\n",
        "\n",
        "print (\"\")\n",
        "print (\"Accuracy Score: \", accuracy_score(val_y, pred_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhfniNn-T6qg",
        "outputId": "76c1fffa-3987-4790-f128-e1e0f93563e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 268ms/step - loss: 0.9914 - val_loss: 0.8748\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9908 - val_loss: 0.8742\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9900 - val_loss: 0.8736\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9895 - val_loss: 0.8730\n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9889 - val_loss: 0.8724\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9882 - val_loss: 0.8718\n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9876 - val_loss: 0.8712\n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9867 - val_loss: 0.8706\n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9860 - val_loss: 0.8700\n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9860 - val_loss: 0.8693\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      1.00      0.99       742\n",
            "         1.0       1.00      0.88      0.93       131\n",
            "\n",
            "    accuracy                           0.98       873\n",
            "   macro avg       0.99      0.94      0.96       873\n",
            "weighted avg       0.98      0.98      0.98       873\n",
            "\n",
            "\n",
            "Accuracy Score:  0.981672394043528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# autoencoder version nosotros"
      ],
      "metadata": {
        "id": "m-8IVgXaZMAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(203)\n",
        "\n",
        "data = pd.read_csv(\"/content/dataset/creditcard.csv\")\n",
        "data[\"Time\"] = data[\"Time\"].apply(lambda x : x / 3600 % 24)\n",
        "\n",
        "non_fraud = data[data['Class'] == 0]\n",
        "fraud = data[data['Class'] == 1]\n",
        "\n",
        "df = pd.concat([non_fraud, fraud]).sample(frac=1).reset_index(drop=True)\n",
        "X = df.drop(['Class'], axis = 1).values\n",
        "Y = df[\"Class\"].values\n",
        "\n",
        "# input layer\n",
        "input_layer = Input(shape=(X.shape[1],))\n",
        "\n",
        "# encoding part\n",
        "encoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
        "encoded = Dense(50, activation='relu')(encoded)\n",
        "\n",
        "# decoding part\n",
        "decoded = Dense(50, activation='tanh')(encoded)\n",
        "decoded = Dense(100, activation='tanh')(decoded)\n",
        "\n",
        "# output layer\n",
        "output_layer = Dense(X.shape[1], activation='relu')(decoded)\n",
        "\n",
        "# model architecture\n",
        "autoencoder = Model(input_layer, output_layer)\n",
        "autoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")\n",
        "\n",
        "x = data.drop([\"Class\"], axis=1)\n",
        "y = data[\"Class\"].values\n",
        "x_scale = preprocessing.MinMaxScaler().fit_transform(x.values)\n",
        "x_norm, x_fraud = x_scale[y == 0], x_scale[y == 1]\n",
        "\n",
        "autoencoder.fit(x_norm,\n",
        "                x_norm,\n",
        "                batch_size = 256,\n",
        "                epochs = 10,\n",
        "                shuffle = True,\n",
        "                validation_split = 0.20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThMrTq4OZO8l",
        "outputId": "d1408927-5aba-4d66-e152-050997926128"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.9161 - val_loss: 0.8349\n",
            "Epoch 2/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.7839 - val_loss: 0.6934\n",
            "Epoch 3/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.6438 - val_loss: 0.5522\n",
            "Epoch 4/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.5103 - val_loss: 0.4396\n",
            "Epoch 5/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.4039 - val_loss: 0.3533\n",
            "Epoch 6/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3201 - val_loss: 0.2805\n",
            "Epoch 7/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.2553 - val_loss: 0.2305\n",
            "Epoch 8/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.2121 - val_loss: 0.1995\n",
            "Epoch 9/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1851 - val_loss: 0.1798\n",
            "Epoch 10/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1678 - val_loss: 0.1673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Latent Representations\n",
        "hidden_representation = Sequential()\n",
        "hidden_representation.add(autoencoder.layers[0])\n",
        "hidden_representation.add(autoencoder.layers[1])\n",
        "hidden_representation.add(autoencoder.layers[2])"
      ],
      "metadata": {
        "id": "q0Ls-9XzcLiU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A: MODELO BASE SIN OVER/UNDER SAMPLING"
      ],
      "metadata": {
        "id": "Vd_gxgYwcN2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the hidden representations of two classes : non-fraud and fraud by predicting the raw inputs\n",
        "norm_hid_rep = hidden_representation.predict(x_norm)\n",
        "fraud_hid_rep = hidden_representation.predict(x_fraud)\n",
        "\n",
        "# create a training dataset using the latent representations\n",
        "rep_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)\n",
        "y_n = np.zeros(norm_hid_rep.shape[0])\n",
        "y_f = np.ones(fraud_hid_rep.shape[0])\n",
        "rep_y = np.append(y_n, y_f)\n",
        "\n",
        "# Linear Classifier\n",
        "train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25)\n",
        "clf = LogisticRegression(solver=\"lbfgs\").fit(train_x, train_y)\n",
        "pred_y = clf.predict(val_x)\n",
        "\n",
        "print (\"\")\n",
        "print (\"Classification Report: \")\n",
        "print (classification_report(val_y, pred_y))\n",
        "\n",
        "print (\"\")\n",
        "print (\"Accuracy Score: \", accuracy_score(val_y, pred_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcLsZg2IaD92",
        "outputId": "01814d04-bac1-4048-8dc0-13185701120d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8885/8885\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00     71080\n",
            "         1.0       0.85      0.37      0.51       122\n",
            "\n",
            "    accuracy                           1.00     71202\n",
            "   macro avg       0.92      0.68      0.76     71202\n",
            "weighted avg       1.00      1.00      1.00     71202\n",
            "\n",
            "\n",
            "Accuracy Score:  0.998806213308615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B: MODELO UNDERSAMPLING"
      ],
      "metadata": {
        "id": "Bf4-TH9lcSS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### APLICAR DE ALGUNA FORMA UNDERSAMPLING AL X_NORM\n",
        "\n",
        "\n",
        "\n",
        "# Generate the hidden representations of two classes : non-fraud and fraud by predicting the raw inputs\n",
        "norm_hid_rep = hidden_representation.predict(x_norm)\n",
        "fraud_hid_rep = hidden_representation.predict(x_fraud)\n",
        "\n",
        "# create a training dataset using the latent representations\n",
        "rep_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)\n",
        "y_n = np.zeros(norm_hid_rep.shape[0])\n",
        "y_f = np.ones(fraud_hid_rep.shape[0])\n",
        "rep_y = np.append(y_n, y_f)\n",
        "\n",
        "# Linear Classifier\n",
        "train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25)\n",
        "clf = LogisticRegression(solver=\"lbfgs\").fit(train_x, train_y)\n",
        "pred_y = clf.predict(val_x)\n",
        "\n",
        "print (\"\")\n",
        "print (\"Classification Report: \")\n",
        "print (classification_report(val_y, pred_y))\n",
        "\n",
        "print (\"\")\n",
        "print (\"Accuracy Score: \", accuracy_score(val_y, pred_y))"
      ],
      "metadata": {
        "id": "pQAnEjYLcd0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oXlp-VbBcc0a"
      }
    }
  ]
}